# === Imports ===
import os
import pickle
import faiss
import numpy as np
import requests
from typing import List
from langdetect import detect
from sentence_transformers import SentenceTransformer
import google.generativeai as genai
from pathlib import Path
from essai import build_index

# === Config API Gemini ===
API_KEY = "AIzaSyC_x5AGH-cdrq1cgZGaPJ7yQFZYzo2UAYg"
API_URL = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={API_KEY}"
genai.configure(api_key=API_KEY)

# === Param√®tres RAG ===
TOP_K = 5
EMBEDDING_MODEL = "all-MiniLM-L6-v2"

# === Chemins des fichiers ===
BASE_DIR = Path(__file__).parent  # dossier du script
INDEX_PATH = BASE_DIR / "rag_output" / "faiss.index"
SENTENCE_PATH = BASE_DIR / "rag_output" / "sentences.pkl"
METADATA_PATH = BASE_DIR / "rag_output" / "metadata.pkl"

# === Fonctions Utilitaires ===

def detect_language(text):
    """D√©tecte la langue d'un texte"""
    try:
        return detect(text)
    except:
        return 'unknown'

def ask_gemini(prompt):
    """Interroge l'API Gemini avec un prompt"""
    headers = {"Content-Type": "application/json"}
    payload = {
        "contents": [{"parts": [{"text": prompt}]}]
    }
    response = requests.post(API_URL, headers=headers, json=payload)
    if response.status_code == 200:
        result = response.json()
        return result['candidates'][0]['content']['parts'][0]['text']
    else:
        return f"‚ùå Error: {response.status_code}, {response.text}"

def load_data():
    """Charge les donn√©es RAG : phrases, metadata et index FAISS"""
    # V√©rifie si les fichiers existent
    if not INDEX_PATH.exists() or not SENTENCE_PATH.exists() or not METADATA_PATH.exists():
        raise FileNotFoundError("Un ou plusieurs fichiers RAG sont introuvables.")

    # Chargement des phrases
    with open(str(SENTENCE_PATH), "rb") as f:
        sentences = pickle.load(f)
    # Chargement des m√©tadonn√©es
    with open(str(METADATA_PATH), "rb") as f:
        metadata = pickle.load(f)
    # Chargement de l'index FAISS
    index = faiss.read_index(str(INDEX_PATH))

    return sentences, metadata, index

def get_top_k_chunks(sentences, index, query_vec, k=TOP_K):
    """Recherche les k chunks les plus proches"""
    D, I = index.search(np.array(query_vec), k)
    return [sentences[i] for i in I[0]]

def generate_answer(context_chunks: List[str], user_prompt: str):
    """Construit le prompt final et r√©cup√®re la r√©ponse Gemini"""
    lang = detect_language(user_prompt)
    if lang == 'en':
        instruction = "Reply in English."
    elif lang == 'fr':
        instruction = "R√©ponds en fran√ßais."
    else:
        instruction = "R√©ponds en fran√ßais m√™me si la langue du prompt n'est pas le fran√ßais."

    context = "\n---\n".join(context_chunks)

    full_prompt = f"""
{instruction}

Tu es un assistant expert en orientation universitaire.
Utilise uniquement les informations suivantes sur les programmes Pristini.

Donne une r√©ponse claire, concise et r√©sum√©e, en mettant en avant les points cl√©s et sans d√©tails superflus.
Si tu ne connais pas la r√©ponse, excuse-toi et demande √† l'utilisateur de reformuler ou d'apporter plus de contexte.
Pr√©sente-toi et r√©ponds aux salutations de l'utilisateur.

Context:
{context}

User question:
{user_prompt}

Answer:
"""

    response = ask_gemini(full_prompt)
    return response

def query_data(user_prompt):
    """Effectue une requ√™te : encode, r√©cup√®re les chunks et g√©n√®re la r√©ponse"""
    print(f"üîé Query: {user_prompt}")

    embedder = SentenceTransformer(EMBEDDING_MODEL)
    query_vec = embedder.encode([user_prompt])

    # Chargement des donn√©es
    sentences, metadata, index = load_data()

    # Recherche des meilleurs chunks
    top_chunks = get_top_k_chunks(sentences, index, query_vec)

    # Sauvegarde des chunks r√©cup√©r√©s pour debug
    with open("pristini_top_chunks.txt", "w", encoding="utf-8") as f:
        for i, chunk in enumerate(top_chunks):
            f.write(f"üîπ Chunk {i+1}:\n{chunk}\n{'-'*80}\n")

    print("‚úÖ Top chunks saved to pristini_top_chunks.txt")

    # G√©n√©ration de la r√©ponse via Gemini
    answer = generate_answer(top_chunks, user_prompt)
    return answer

# === Run CLI ===
if __name__ == "__main__":
    user_question = input("Ask any Pristini-related question: ")
    result = query_data(user_question)
    print("\nüí° LLM Response:\n", result)

    with open("pristini_llm_response.txt", "w", encoding="utf-8") as f:
        f.write("üí° LLM Response:\n")
        f.write(result)
